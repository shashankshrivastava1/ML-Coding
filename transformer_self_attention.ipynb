{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "043264b4-b69d-4a32-8341-797457d099ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f38c1697-40e5-4162-b3bb-c2806b61f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 8\n",
    "t = 4\n",
    "\n",
    "inputVector = np.random.randn(dim,t) #column vectors, each column in a word/token, t number of words/tokens\n",
    "\n",
    "Wq,Wk,Wv = [np.random.randn(dim,dim) for _ in range(3)]\n",
    "# Wq,Wk,Wv weight matrix/ learnable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6b22d6-ca1f-4e5f-9d8d-b54f511ef194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07ef3be7-7e13-49b7-814d-bba6c25e3c14",
   "metadata": {},
   "source": [
    "selfattention = np.matmul(softmax(Q.T, K)/np.sqrt(dim) , V.T)\n",
    "\n",
    "$$ softmax\\bigg(\\dfrac{Q^{T} . K}{ \\sqrt{d_k}}\\bigg) . V^{T}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "165ea82b-07cb-415d-b1da-ed238831e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is doing coloumn wise softmax\n",
    "def softmax(x):\n",
    "    exp =np.exp(x)\n",
    "    return exp/ np.sum(exp, axis=0)\n",
    "# a = np.array([[1,2,3],[1,2,3],[1,2,3]])\n",
    "# softmax(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d3347a1-4906-4693-a406-3e89c16aa61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.76660450e+00,  1.47641852e+00,  8.89064677e-01,\n",
       "         6.58230293e+00, -2.53293759e+00, -2.81823058e-01,\n",
       "        -1.05722873e+00,  6.43103087e+00],\n",
       "       [-8.45617516e-03, -3.14115339e-03, -3.01421736e-02,\n",
       "        -2.47553761e-05,  8.44594894e-03, -1.51827238e-02,\n",
       "         9.22010590e-03,  1.43945159e-02],\n",
       "       [ 1.11143443e+00,  2.88957066e-01, -2.43045761e+00,\n",
       "         1.73726701e-01,  1.88817831e+00,  3.04025846e+00,\n",
       "         1.17736817e+00,  4.43870375e+00],\n",
       "       [-3.83746470e+00, -1.04980661e+00, -5.26408077e+00,\n",
       "        -2.38831001e+00,  2.30226542e+00, -2.38723966e+00,\n",
       "         1.89465947e+00,  3.06360121e-02]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def self_attention(inputVector):\n",
    "    Q = np.dot(Wq,inputVector) # (dim,dim) * (dim,t) = (dim,t)\n",
    "    K = np.dot(Wk,inputVector)\n",
    "    V = np.dot(Wv,inputVector)\n",
    "    matmul = np.matmul(Q.T,K)\n",
    "    matmul = matmul / np.sqrt(dim) # (t,dim) (dim,t) =(t,t)\n",
    "    \n",
    "    temp = softmax(matmul)\n",
    "    \n",
    "                   \n",
    "    self_attention = np.dot(temp,V.T) # (t,t) (t,d) = (t, d)\n",
    "    return self_attention\n",
    "self_attention(inputVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b19066-5051-428c-80d3-b0a7b552be4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "382a8d1b-3046-4b4a-9e08-d517f2f1c450",
   "metadata": {},
   "source": [
    "$$ softmax( \\frac{Q^T . K}{\\sqrt{d_k}}) . V^T $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a752faf8-4baa-41d1-a245-c578f60628ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "af07cc51-22c5-4cf5-b78d-d99ed3e0a41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.89975040e-03, 8.75600595e-01, 1.18499655e-01],\n",
       "       [9.99999953e-01, 4.13993752e-08, 5.60279617e-09]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_new(x):\n",
    "    exp = np.exp(x)\n",
    "    # print(exp)\n",
    "    return exp / np.sum(exp,axis=1).reshape(-1,1)\n",
    "\n",
    "a = np.array([[0,5,3],[22,5,3]])\n",
    "softmax_new(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a70164c9-5742-4c43-85ae-d043935ad356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 8) (32, 8) (32, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, 32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_weight(H):\n",
    "    Wq = np.random.randn(H.shape[0],H.shape[0]) * 0.001\n",
    "    Wk = np.random.randn(H.shape[0],H.shape[0]) * 0.001\n",
    "    Wv = np.random.randn(H.shape[0],H.shape[0]) * 0.001\n",
    "    return Wq,Wk,Wv\n",
    "\n",
    "def input_layer(H):\n",
    "    Wq,Wk,Wv =initialize_weight(H)\n",
    "    Q = np.dot(Wq,H)\n",
    "    K = np.dot(Wk,H)\n",
    "    V = np.dot(Wv,H)\n",
    "    return Q,K,V\n",
    "    \n",
    "def selfattention(Q,K,V):\n",
    "    self_attention = np.dot(Q.T,K) \n",
    "    self_attention = self_attention / np.sqrt(K.shape[0])\n",
    "    self_attention = softmax_new(self_attention)\n",
    "    self_attention = np.dot(self_attention, V.T)\n",
    "    return self_attention\n",
    "\n",
    "H = np.random.randn(32,8)\n",
    "\n",
    "Q,K,V = input_layer(H)\n",
    "\n",
    "print(Q.shape,K.shape,V.shape)\n",
    "\n",
    "\n",
    "selfattention(Q,K,V).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda48bb4-e111-4984-bdc7-82f4605cbc05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtl_intent",
   "language": "python",
   "name": "mtl_intent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
