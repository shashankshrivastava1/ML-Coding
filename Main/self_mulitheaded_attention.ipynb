{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "feaaa743-e2ee-4e3c-996f-7b3831f80b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "seq_lenght = 4\n",
    "d_model = 8\n",
    "num_head =2 \n",
    "X = np.random.randn(seq_lenght,d_model)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b983ed9d-50a4-4448-b35b-b93892d50f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp,axis=1,keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c0420a5-caf9-4c42-a925-989e361d9a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 4)\n",
      "(2, 4, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multiheaded_attention(X,num_head):\n",
    "    seq_length,d_model = X.shape\n",
    "    d_head = d_model//num_head\n",
    "\n",
    "    Wq = np.random.randn(d_model, d_model)\n",
    "    Wk = np.random.randn(d_model, d_model)\n",
    "    Wv = np.random.randn(d_model, d_model)\n",
    "    Wo = np.random.randn(d_model, d_model)\n",
    "\n",
    "    Q = np.dot(X,Wq)\n",
    "    K = np.dot(X,Wk)\n",
    "    V = np.dot(X,Wv)\n",
    "    O = np.dot(X,Wo)\n",
    "\n",
    "    Q = Q.reshape(seq_lenght,num_head,d_head).transpose(1,0,2)\n",
    "    K = K.reshape(seq_lenght,num_head,d_head).transpose(1,0,2)\n",
    "    V = V.reshape(seq_lenght,num_head,d_head).transpose(1,0,2)\n",
    "    print(Q.shape)\n",
    "    #Scaled dot product\n",
    "    scores = np.matmul(Q, K.transpose(0,2,1)) / np.sqrt(d_head)\n",
    "    attention_weights = softmax(scores)\n",
    "    attention_output = np.matmul(attention_weights,V)\n",
    "    print(attention_output.shape)\n",
    "    attention_output = attention_output.transpose(1,0,2).reshape(seq_lenght,d_model)\n",
    "\n",
    "    output = np.dot(attention_output,Wo)\n",
    "    return output\n",
    "\n",
    "multiheaded_attention(X,num_head).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca88b5e8-dd10-4a45-947a-0c462eeab904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape #4 words, 8 dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d710bcb5-8717-44a8-bc67-a37cda04c50f",
   "metadata": {},
   "source": [
    "$$ softmax( \\frac{Q.K^T}{\\sqrt{d_k}}) . V $$\n",
    "\n",
    "$$ softmax( \\frac{Q^T.K}{\\sqrt{d_k}}) . V^T $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "225290b4-eede-4fc0-ba1b-45c283a0e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp, axis =1, keepdims=True)\n",
    "\n",
    "a = np.array([[1,6,2],[9,22,-2]])\n",
    "print(softmax(a).sum(axis =1,keepdims=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e9ab918-20ec-4d5a-b181-799fd8a03343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#row wise arragend data\n",
    "def self_attention(q,k,v):\n",
    "    d = k.shape[-1]\n",
    "    scores = np.matmul(q, k.T)\n",
    "    scaled_scores = scores / np.sqrt(d)\n",
    "    scaled_scores = softmax(scaled_scores)\n",
    "    return np.matmul(scaled_scores,v)\n",
    "#row wise arragend data\n",
    "def self_attention_multiheaded(q,k,v):\n",
    "    d = k.shape[-1]\n",
    "    scores = np.matmul(q, k.transpose(0,2,1)) # (2,4,4) * (2,4,4)\n",
    "    scaled_scores = scores / np.sqrt(d)\n",
    "    scaled_scores = softmax(scaled_scores)\n",
    "    return np.matmul(scaled_scores,v)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6782c8e8-346f-4320-adaf-a167a9831558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randn(seq_lenght,d_model)\n",
    "x.shape\n",
    "Wq = np.random.randn(x.shape[1],x.shape[1])\n",
    "Wk = np.random.randn(x.shape[1],x.shape[1])\n",
    "Wv =  np.random.randn(x.shape[1],x.shape[1])\n",
    "Wo = np.random.randn(x.shape[1],x.shape[1])\n",
    "def multiheaded_attention(H,num_heads):\n",
    "    \"\"\"\n",
    "        H: dim (t,d)\n",
    "        output: dim(t,d)\n",
    "    \"\"\"\n",
    "    t = H.shape[0]\n",
    "    d_head = H.shape[1]//num_heads\n",
    "    q = np.matmul(H,Wq)\n",
    "    k = np.matmul(H,Wk)\n",
    "    v = np.matmul(H,Wv)\n",
    "    o = np.matmul(H,Wo)\n",
    "\n",
    "    q = q.reshape(t,num_heads,d_head).transpose(1,0,2)\n",
    "    k = k.reshape(t,num_heads,d_head).transpose(1,0,2)\n",
    "    v = v.reshape(t,num_heads,d_head).transpose(1,0,2)\n",
    "    o = o.reshape(t,num_heads,d_head).transpose(1,0,2)\n",
    "\n",
    "    scaled_dot_product = self_attention_multiheaded(q,k,v)\n",
    "    print(scaled_dot_product.shape)\n",
    "\n",
    "    scaled_dot_product = scaled_dot_product.transpose(1,0,2).reshape(t,H.shape[1])\n",
    "    return np.matmul(scaled_dot_product,Wo)\n",
    "    \n",
    "    \n",
    "M1 = multiheaded_attention(x,2)\n",
    "M1.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe6f785-637c-430e-bfaf-57daeb5ded7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
