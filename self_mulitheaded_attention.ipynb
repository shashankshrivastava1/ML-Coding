{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feaaa743-e2ee-4e3c-996f-7b3831f80b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "seq_lenght = 4\n",
    "d_model = 8\n",
    "num_head =2 \n",
    "X = np.random.randn(seq_lenght,d_model)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b983ed9d-50a4-4448-b35b-b93892d50f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp,axis=1,keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c0420a5-caf9-4c42-a925-989e361d9a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 4)\n",
      "(2, 4, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multiheaded_attention(X,num_head):\n",
    "    seq_length,d_model = X.shape\n",
    "    d_head = d_model//num_head\n",
    "\n",
    "    Wq = np.random.randn(d_model, d_model)\n",
    "    Wk = np.random.randn(d_model, d_model)\n",
    "    Wv = np.random.randn(d_model, d_model)\n",
    "    Wo = np.random.randn(d_model, d_model)\n",
    "\n",
    "    Q = np.dot(X,Wq)\n",
    "    K = np.dot(X,Wk)\n",
    "    V = np.dot(X,Wv)\n",
    "    O = np.dot(X,Wo)\n",
    "\n",
    "    Q = Q.reshape(seq_lenght,num_head,d_head).transpose(1,0,2)\n",
    "    K = K.reshape(seq_lenght,num_head,d_head).transpose(1,0,2)\n",
    "    V = V.reshape(seq_lenght,num_head,d_head).transpose(1,0,2)\n",
    "    print(Q.shape)\n",
    "    #Scaled dot product\n",
    "    scores = np.matmul(Q, K.transpose(0,2,1)) / np.sqrt(d_head)\n",
    "    attention_weights = softmax(scores)\n",
    "    attention_output = np.matmul(attention_weights,V)\n",
    "    print(attention_output.shape)\n",
    "    attention_output = attention_output.transpose(1,0,2).reshape(seq_lenght,d_model)\n",
    "\n",
    "    output = np.dot(attention_output,Wo)\n",
    "    return output\n",
    "\n",
    "multiheaded_attention(X,num_head).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9ab918-20ec-4d5a-b181-799fd8a03343",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
