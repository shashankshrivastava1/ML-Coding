{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15003ec4-dbc2-43d3-a072-b121e9c7e37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiHeadAttention:\n",
    "    \"\"\"\n",
    "    Multi-head attention.\n",
    "\n",
    "    Parameters:\n",
    "    num_hiddens: int\n",
    "        Number of hidden units.\n",
    "    num_heads: int\n",
    "        Number of attention heads.\n",
    "    dropout: float\n",
    "        Dropout rate.\n",
    "    bias: bool\n",
    "        Whether to include bias parameters in the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_hiddens, num_heads, dropout=0.0, bias=False):\n",
    "        self.num_heads = num_heads\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.d_k = self.d_v = num_hiddens // num_heads\n",
    "        \n",
    "        self.W_q = np.random.rand(num_hiddens, num_hiddens)\n",
    "        self.W_k = np.random.rand(num_hiddens, num_hiddens)\n",
    "        self.W_v = np.random.rand(num_hiddens, num_hiddens)\n",
    "        self.W_o = np.random.rand(num_hiddens, num_hiddens)\n",
    "        \n",
    "        if bias:\n",
    "            self.b_q = np.random.rand(num_hiddens)\n",
    "            self.b_k = np.random.rand(num_hiddens)\n",
    "            self.b_v = np.random.rand(num_hiddens)\n",
    "            self.b_o = np.random.rand(num_hiddens)\n",
    "        else:\n",
    "            self.b_q = self.b_k = self.b_v = self.b_o = np.zeros(num_hiddens)\n",
    "\n",
    "    def transpose_qkv(self, X):\n",
    "        \"\"\"\n",
    "        Transposition for batch processing\n",
    "        \n",
    "        Parameters:\n",
    "        X: np.ndarray\n",
    "            Input tensor\n",
    "\n",
    "        Returns:\n",
    "        np\n",
    "            Transposed tensor\n",
    "        \"\"\"\n",
    "        X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n",
    "        X = X.transpose(0, 2, 1, 3)\n",
    "        return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "    \n",
    "    def transpose_output(self, X):\n",
    "        \"\"\"\n",
    "        Transposition for output\n",
    "\n",
    "        Parameters:\n",
    "        X: np.ndarray\n",
    "            Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        np\n",
    "            Transposed tensor\n",
    "        \"\"\"\n",
    "        X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n",
    "        X = X.transpose(0, 2, 1, 3)\n",
    "        return X.reshape(X.shape[0], X.shape[1], -1)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, valid_lens):\n",
    "        \"\"\"\n",
    "        Scaled dot product attention\n",
    "\n",
    "        Parameters:\n",
    "        Q: np.ndarray\n",
    "            Query tensor\n",
    "        K: np.ndarray\n",
    "            Key tensor\n",
    "        V: np.ndarray\n",
    "            Value tensor\n",
    "        valid_lens: np.ndarray\n",
    "            Valid lengths for the query\n",
    "        \n",
    "        Returns:\n",
    "        np\n",
    "            Output tensor\n",
    "        \"\"\"\n",
    "        d_k = Q.shape[-1]\n",
    "        scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n",
    "        if valid_lens is not None:\n",
    "            mask = np.arange(scores.shape[-1]) < valid_lens[:, None]\n",
    "            scores = np.where(mask[:, None, :], scores, -np.inf)\n",
    "        attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
    "        attention_weights /= attention_weights.sum(axis=-1, keepdims=True)\n",
    "        return np.matmul(attention_weights, V)\n",
    "    \n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "\n",
    "        Parameters:\n",
    "        queries: np.ndarray\n",
    "            Query tensor\n",
    "        keys: np.ndarray\n",
    "            Key tensor\n",
    "        values: np.ndarray\n",
    "            Value tensor\n",
    "        valid_lens: np.ndarray\n",
    "            Valid lengths for the query\n",
    "\n",
    "        Returns:\n",
    "        np\n",
    "            Output tensor\n",
    "        \"\"\"\n",
    "        print(\"np.dot(queries, self.W_q).shape\",np.dot(queries, self.W_q).shape,\"self.W_q.shape\",self.W_q.shape)\n",
    "        queries = self.transpose_qkv(np.dot(queries, self.W_q) + self.b_q)\n",
    "        print(\"queries\",queries.shape)\n",
    "        keys = self.transpose_qkv(np.dot(keys, self.W_k) + self.b_k)\n",
    "        print(\"keys\",keys.shape)\n",
    "        values = self.transpose_qkv(np.dot(values, self.W_v) + self.b_v)\n",
    "        print(\"values\",values.shape)\n",
    "        \n",
    "        \n",
    "        if valid_lens is not None:\n",
    "            valid_lens = np.repeat(valid_lens, self.num_heads, axis=0)\n",
    "        \n",
    "        output = self.scaled_dot_product_attention(queries, keys, values, valid_lens)\n",
    "        output_concat = self.transpose_output(output)\n",
    "        return np.dot(output_concat, self.W_o) + self.b_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d02bf76-0b8e-4c54-9c7a-c2854e359016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 100) (2, 6, 100)\n",
      "np.dot(queries, self.W_q).shape (2, 4, 100) self.W_q.shape (100, 100)\n",
      "queries (10, 4, 20)\n",
      "keys (10, 6, 20)\n",
      "values (10, 6, 20)\n",
      "Output shape: (2, 4, 100)\n",
      "Output data: (2, 4, 100)\n"
     ]
    }
   ],
   "source": [
    "# Define dimensions and initialize multi-head attention\n",
    "### d = num_hiddens\n",
    "### t = num_queries\n",
    "### batch_size = batch_size\n",
    "\n",
    "num_hiddens, num_heads = 100, 5\n",
    "attention = MultiHeadAttention(num_hiddens, num_heads, dropout=0.5, bias=False)\n",
    "\n",
    "# Define sample data\n",
    "batch_size, num_queries, num_kvpairs = 2, 4, 6\n",
    "valid_lens = np.array([3, 2])\n",
    "X = np.random.rand(batch_size, num_queries, num_hiddens)  # Use random data to simulate input queries\n",
    "Y = np.random.rand(batch_size, num_kvpairs, num_hiddens)  # Use random data to simulate key-value pairs\n",
    "print(X.shape,Y.shape)\n",
    "\n",
    "# Apply multi-head attention\n",
    "output = attention.forward(X, Y, Y, valid_lens)\n",
    "print(\"Output shape:\", output.shape)  # Expected shape: (batch_size, num_queries, num_hiddens)\n",
    "\n",
    "# Display the output for inspection\n",
    "print(\"Output data:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aecc96ab-e402-4fe5-a1ba-300c21288e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q.shape before transform (4, 8)\n",
      "Q.shape after transform (2, 4, 4)\n",
      "scores (2, 4, 4)\n",
      "attention_output (2, 4, 4)\n",
      "Output shape: (4, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# perplexity\n",
    "def multihead_attention(query, key, value, num_heads, d_model):\n",
    "    seq_length, _ = query.shape\n",
    "    d_head = d_model // num_heads\n",
    "\n",
    "    # Linear projections\n",
    "    W_q = np.random.randn(d_model, d_model)\n",
    "    W_k = np.random.randn(d_model, d_model)\n",
    "    W_v = np.random.randn(d_model, d_model)\n",
    "    W_o = np.random.randn(d_model, d_model)\n",
    "\n",
    "    Q = np.dot(query, W_q)\n",
    "    K = np.dot(key, W_k)\n",
    "    V = np.dot(value, W_v)\n",
    "    print(\"Q.shape before transform\",Q.shape)\n",
    "    # Reshape for multihead attention\n",
    "    Q = Q.reshape(seq_length, num_heads, d_head).transpose(1, 0, 2)\n",
    "    print(\"Q.shape after transform\",Q.shape)\n",
    "    K = K.reshape(seq_length, num_heads, d_head).transpose(1, 0, 2)\n",
    "    V = V.reshape(seq_length, num_heads, d_head).transpose(1, 0, 2)\n",
    "\n",
    "    # Scaled dot-product attention\n",
    "    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_head)\n",
    "    print(\"scores\",scores.shape)\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    attention_output = np.matmul(attention_weights, V)\n",
    "    print(\"attention_output\",attention_output.shape)\n",
    "\n",
    "    # Reshape and concatenate heads\n",
    "    attention_output = attention_output.transpose(1, 0, 2).reshape(seq_length, d_model)\n",
    "\n",
    "    # Final linear projection\n",
    "    output = np.dot(attention_output, W_o)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "seq_length = 4\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "\n",
    "query = np.random.randn(seq_length, d_model)\n",
    "key = np.random.randn(seq_length, d_model)\n",
    "value = np.random.randn(seq_length, d_model)\n",
    "\n",
    "output = multihead_attention(query, key, value, num_heads, d_model)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d845a1d2-7a26-4546-92f1-dee53907e8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
